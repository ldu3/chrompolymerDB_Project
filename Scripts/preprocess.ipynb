{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bx.bbi.bigwig_file import BigWigFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate human chromosome size data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromosome sizes saved to /Users/siyuanzhao/Documents/GitHub/CS522_Project/Scripts/chromosome_sizes.txt\n"
     ]
    }
   ],
   "source": [
    "def fetch_chrom_sizes(genome, output_file):\n",
    "    \"\"\"Utility for downloading chromosome sizes from UCSC.\"\"\"\n",
    "    full_file_path = os.path.join(os.getcwd(), output_file)\n",
    "\n",
    "    url = f\"http://hgdownload.soe.ucsc.edu/goldenPath/{genome}/bigZips/{genome}.chrom.sizes\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(full_file_path, \"w\") as f:\n",
    "            for line in response.text.splitlines():\n",
    "                chrom, size = line.split()\n",
    "                f.write(f\"{chrom}\\t{size}\\n\")\n",
    "        print(f\"Chromosome sizes saved to {full_file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch chromosome sizes for {genome}\")\n",
    "\n",
    "\n",
    "output_file = \"chromosome_sizes.txt\"\n",
    "fetch_chrom_sizes(\"hg38\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate folding input files**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def make_dir(d):\n",
    "    \"\"\"Utility for making a directory if not existing.\"\"\"\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "def get_spe_inter(hic_data, alpha=0.05):\n",
    "    \"\"\"Filter Hi-C data for significant interactions based on the alpha threshold.\"\"\"\n",
    "    hic_spe = hic_data.loc[hic_data['fdr'] < alpha]\n",
    "    return hic_spe\n",
    "\n",
    "def get_fold_inputs(spe_df):\n",
    "    \"\"\"Prepare folding input file from the filtered significant interactions.\"\"\"\n",
    "    spe_out_df = spe_df[['ibp', 'jbp', 'fq', 'chr', 'fdr']]\n",
    "    spe_out_df['w'] = 1\n",
    "    result = spe_out_df[['chr', 'ibp', 'jbp', 'fq', 'w']]\n",
    "    return result\n",
    "\n",
    "def process_hic_files(input_folder, seqs_folder, output_folder, alpha=0.05):\n",
    "    \"\"\"Process Hi-C files by matching with seqs files for reference data and save results in the output folder.\"\"\"\n",
    "    \n",
    "    make_dir(output_folder)\n",
    "    \n",
    "    # Iterate through each file in the Hi-C input folder\n",
    "    for hic_file_name in os.listdir(input_folder):\n",
    "        if hic_file_name.endswith(\".csv.gz\"):\n",
    "            key_word = hic_file_name.split(\"_\")[0]  # Assume keyword is the prefix before the first underscore\n",
    "            \n",
    "            # Find the matching reference file in the seqs folder\n",
    "            seq_file_name = f\"{key_word}_ranges.csv.gz\"\n",
    "            seq_file_path = os.path.join(seqs_folder, seq_file_name)\n",
    "            \n",
    "            if not os.path.exists(seq_file_path):\n",
    "                print(f\"No matching reference file found for {hic_file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Load Hi-C data and the corresponding reference data\n",
    "            hic_file_path = os.path.join(input_folder, hic_file_name)\n",
    "            all_hic = pd.read_csv(hic_file_path)\n",
    "            spe_hic = get_spe_inter(all_hic, alpha)\n",
    "            reference_df = pd.read_csv(seq_file_path, usecols=[\"chrID\", \"start_value\", \"end_value\"])\n",
    "\n",
    "            # Process each row in the reference file for filtering\n",
    "            for _, row in reference_df.iterrows():\n",
    "                chrID = row[\"chrID\"]\n",
    "                start_value = row[\"start_value\"]\n",
    "                end_value = row[\"end_value\"]\n",
    "                \n",
    "                # Filter Hi-C data based on chrID and ibp range\n",
    "                chr_hic_data = spe_hic[\n",
    "                    (spe_hic[\"chr\"] == chrID) &\n",
    "                    (spe_hic[\"ibp\"] >= start_value) &\n",
    "                    (spe_hic[\"ibp\"] <= end_value)\n",
    "                ]\n",
    "                \n",
    "                if chr_hic_data.empty:\n",
    "                    continue\n",
    "\n",
    "                # Prepare folding input data\n",
    "                fold_hic = get_fold_inputs(chr_hic_data)\n",
    "\n",
    "                # Generate output file name and save\n",
    "                output_file_name = f\"{key_word}.{chrID}.{start_value}.{end_value}.txt\"\n",
    "                fold_hic_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "                fold_hic.to_csv(fold_hic_path, header=None, index=None, sep=\"\\t\", mode=\"a\")\n",
    "\n",
    "input_folder = '../Data/refined_processed_HiC'\n",
    "seqs_folder = '../Data/seqs'\n",
    "output_folder = '../Data/Folding_input'\n",
    "process_hic_files(input_folder, seqs_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epigenitic tracks**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
